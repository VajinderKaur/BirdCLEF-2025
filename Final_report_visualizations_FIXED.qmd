---
title: "Final Report"
format: html
execute:
  isolated: false
  echo: true
  eval: false
  warning: true
  message: true
---

```{r setup}
# First, check if required packages are installed and install if needed
required_packages <- c("reticulate", "knitr", "rmarkdown")
new_packages <- required_packages[!required_packages %in% installed.packages()[,"Package"]]
if(length(new_packages)) install.packages(new_packages)

# Load reticulate
library(reticulate)

# Error handling for Python environment setup
tryCatch({
  # Check if virtualenv exists and create if needed
  if (!virtualenv_exists("r-reticulate")) {
    message("Creating virtual environment 'r-reticulate'...")
    virtualenv_create("r-reticulate")
  }
  
  # Use the environment but don't require it
  use_virtualenv("r-reticulate", required = FALSE)
  
  # Install packages with pip and error handling
  message("Installing required Python packages...")
  py_install(c("pandas", "numpy", "scipy", "matplotlib"), pip = TRUE)
  
  # Install packages one by one to better handle errors
  tryCatch(py_install("torch", pip = TRUE),
           error = function(e) message("Error installing torch: ", e$message))
  
  tryCatch(py_install("librosa", pip = TRUE),
           error = function(e) message("Error installing librosa: ", e$message))
  
  tryCatch(py_install("opencv-python-headless", pip = TRUE),
           error = function(e) message("Error installing opencv: ", e$message))
  
}, error = function(e) {
  message("Error setting up Python environment: ", e$message)
  message("Will try to continue with existing setup.")
})

# Print Python configuration for debugging
cat("Python configuration:\n")
py_config()
```

```{python}
# Diagnostic imports - see what's available
import sys
import os

# Print Python environment info
print(f"Python version: {sys.version}")
print(f"Python executable: {sys.executable}")

# Try importing each dependency individually with error handling
libraries = ["pandas", "numpy", "torch", "matplotlib"]
for lib in libraries:
    try:
        __import__(lib)
        print(f"Successfully imported {lib}")
    except Exception as e:
        print(f"Failed to import {lib}: {e}")
```

## Executive Summary

This report documents our local inference pipeline for the BirdCLEF 2025 challenge, focused on detecting bird species from soundscape audio. It is based on a high-performing Kaggle notebook with several enhancements.

Key highlights:
- **Model**: EfficientNet backbone with FocalLossBCE, trained across 4 folds.
- **Input**: 5-second mel spectrogram segments created from 32 kHz soundscapes.
- **Prediction**: Ensemble inference with optional test-time augmentation.
- **Output**: Ranked species predictions saved to `submission.csv`.
- **OOF Scores**: Averaged around **0.962**, indicating strong validation performance.

## BirdCLEF 2025 Inference Report

This report documents the inference phase of our solution for the BirdCLEF 2025 challenge. The goal of this competition is to automatically detect bird species from audio soundscapes using machine learning techniques.

We base our work on a high-performing public notebook and introduce modifications to improve prediction accuracy. This file focuses specifically on how we structured the inference pipeline, handled audio preprocessing, implemented the prediction model, and evaluated results.

Key components of this report include:
- Overview of the data and preprocessing steps
- Description of the model inference pipeline
- Visualizations to understand prediction distributions
- Summary of detected species and confidence thresholds

## **Note : We are using a public notebook as a baseline with some edits of our own to improve the accuracy. This file contains additional edits.

### **INFO**

This report presents the results of a local inference pipeline for the **BirdCLEF 2025 challenge**, which involves identifying bird species from soundscape audio using deep learning.

We adapted and extended a high-performing public Kaggle notebook originally developed by [Kadircan İdrisoğlu](https://www.kaggle.com/code/kadircandrisolu/transforming-audio-to-mel-spec-birdclef-25), incorporating:
- A local training and inference setup using EfficientNet models
- Custom audio preprocessing (mel spectrograms, test-time augmentation)
- An ensemble of 4 models trained using FocalLossBCE
- A post-processing step to smooth predictions across time segments

### Key Metrics:
- Out-of-fold validation scores:
  - Fold 0: 0.9652
  - Fold 1: 0.9605
  - Fold 2: 0.9607
  - Fold 3: 0.9626
  - **OOF Average:** 0.9622

In \[1\]:

```{python eval=TRUE, context='global'}
import os
import gc
import warnings
import logging
import time
import math
from pathlib import Path

import numpy as np
import pandas as pd
import types
import librosa
import torch
import torch.nn.functional as F
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import torch.nn.functional as F
import timm
from tqdm.auto import tqdm
import torchvision
warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.ERROR)
```

In \[2\]:
##FocalLossBCE Use
To address the inherent class imbalance in multi-label bird call detection, we implemented a custom loss function named FocalLossBCE. This function combines two components:

Binary Cross-Entropy (BCE): A standard loss for binary classification that penalizes incorrect predictions.

Focal Loss: A more advanced loss that down-weights easy examples and focuses training on harder, misclassified ones.

Our implementation allows tuning of:

alpha: Balances the importance of positive vs. negative examples.

gamma: Controls how much to focus on difficult examples.

bce_weight and focal_weight: Let us blend standard BCE and Focal Loss to benefit from both.

This combined loss is particularly well-suited for imbalanced multi-label problems like ours, where many species appear infrequently but should still be detected accurately. The use of torchvision.ops.focal_loss.sigmoid_focal_loss ensures numerical stability and efficient computation during training.

``` {python eval = TRUE, context = 'global'}        

 
class FocalLossBCE(torch.nn.Module):
    def __init__(
            self,
            alpha: float = 0.25,
            gamma: float = 2,
            reduction: str = "mean",
            bce_weight: float = 0.6,
            focal_weight: float = 1.4,
    ):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        self.bce = torch.nn.BCEWithLogitsLoss(reduction=reduction)
        self.bce_weight = bce_weight
        self.focal_weight = focal_weight

    def forward(self, logits, targets):
        focall_loss = torchvision.ops.focal_loss.sigmoid_focal_loss(
            inputs=logits,
            targets=targets,
            alpha=self.alpha,
            gamma=self.gamma,
            reduction=self.reduction,
        )
        bce_loss = self.bce(logits, targets)
        return self.bce_weight * bce_loss + self.focal_weight * focall_loss
```
```{python eval = TRUE, context = 'global'}

def get_criterion(cfg):
    return FocalLossBCE()
```

In \[3\]:

##️ Visual Index

- [Configuration Setup](#configuration-setup)
- [Taxonomy Overview](#taxonomy-overview)
- [Taxonomy Visualization](#taxonomy-visualization)
- [Audio Preprocessing](#audio-preprocessing)
- [Model Definition](#model-definition)
- [Spectrogram Visualization](#spectrogram-visualization)
- [Final Output Summary](#final-output-summary)
- [Conclusion](#conclusion)

##️ Configuration Setup

The `CFG` class defines all key hyperparameters and file paths used during the inference stage. This includes audio processing parameters such as FFT size, mel bins, and sampling rate, along with model configuration and file paths for the taxonomy and soundscape data.

Important settings include:
- **Sampling rate**: 32,000 Hz
- **Mel Spectrogram size**: 512 bands
- **Inference threshold**: 0.5 for classifying bird presence
- **Window size**: 5 seconds for segmenting soundscapes

```{python eval=TRUE, context='global'}
## Configuration Setup

class CFG:
    # Audio Parameters
    N_FFT = 2048
    HOP_LENGTH = 512
    N_MELS = 512
    FMIN = 20
    FMAX = 16000
    TARGET_SHAPE = (256, 256)
    FS = 32000  
    WINDOW_SIZE = 5

    # Model info
    class CFG:
      model_path = '/Users/reesemullen/Desktop/BU Work/MA 679/BirdCLEF-2025/models'
      model_name = model_name = 'efficientnet_b0'
      use_specific_folds = False
      folds = [0, 1, 2, 3]
      in_channels = 1
      device = 'cpu'


    # File paths - adjust these to your local setup
    test_soundscapes = "/Users/reesemullen/Desktop/BU Work/MA 679/BirdCLEF-2025/birdclef-2025-data/test_soundscapes"
    submission_csv = "/Users/reesemullen/Desktop/BU Work/MA 679/BirdCLEF-2025/birdclef-2025-data/sample_submission.csv"
    taxonomy_csv = "/Users/reesemullen/Desktop/BU Work/MA 679/BirdCLEF-2025/birdclef-2025-data/taxonomy.csv"

    # Inference params
    batch_size = 16
    use_tta = False
    tta_count = 3
    threshold = 0.5
    debug = False
    debug_count = 3

cfg = CFG()
cfg.RANDOM_SEGMENT = True
```

In \[4\]:

## Data Summary: Taxonomy
The taxonomy dataset provides the full list of bird species considered in the BirdCLEF 2025 challenge, along with their scientific classifications. Each row represents a species, identified by a unique primary_label, and includes metadata such as common names, scientific names, family, order, and class. This information is critical not only for interpreting prediction outputs but also for understanding how species are distributed across biological groupings. We summarize key properties of the dataset—like total number of species and the number of distinct classes—and visualize class-level distributions to detect potential imbalances that might influence model bias during prediction.
```{python eval=TRUE, context='global'}
# Load taxonomy data if file exists
try:
    if os.path.exists(cfg.taxonomy_csv):
        taxonomy_df = pd.read_csv(cfg.taxonomy_csv)
        species_ids = taxonomy_df['primary_label'].tolist()
        num_classes = len(species_ids)
        print(f"Number of classes: {num_classes}")
        
        # Summary of the taxonomy dataframe
        print("Taxonomy Shape:", taxonomy_df.shape)
        print("Unique species:", taxonomy_df['primary_label'].nunique())
        
        # Display class distribution if available
        if 'class_name' in taxonomy_df.columns:
            plt.figure(figsize=(10, 6))
            taxonomy_df['class_name'].value_counts().plot(kind='bar', color='teal')
            plt.title("Species Counts by Class")
            plt.ylabel("Count")
            plt.xlabel("Class")
            plt.tight_layout()
            plt.savefig('species_by_class.png')
            plt.close()
            print("Created species distribution chart")
    else:
        print(f"Taxonomy file not found at {cfg.taxonomy_csv}")
        # Create dummy data for demonstration
        species_ids = [f'species_{i}' for i in range(20)]
        num_classes = len(species_ids)
except Exception as e:
    print(f"Error loading taxonomy: {e}")
    # Create dummy data
    species_ids = [f'species_{i}' for i in range(20)]
    num_classes = len(species_ids)
```

## Visual: Species Count by Class

This bar chart visualizes the number of species per biological class in the dataset. Uneven class distributions may affect model confidence and prediction tendencies. For example, if one class dominates, the model might be biased toward it unless handled with proper balancing during training.

```{python eval=TRUE, context='global'}
taxonomy_df['class_name'].value_counts().plot(kind='bar', color='teal')
plt.title("Species Counts by Class")
plt.ylabel("Count")
plt.xlabel("Class")
plt.tight_layout()
plt.show()
```

## Visual: Top 10 Species by Average Model Confidence

The visualizations below provide insight into the model’s behavior during inference:

- **Top 10 Species by Average Confidence**: This chart shows which species the model is most confident about across all predictions. High average confidence may indicate the species has distinctive acoustic features or more training examples.
- **Caution on High Confidence**: High average confidence does not always mean high accuracy. Some false positives may have strong confidence scores if background noise resembles a bird call.
- **Role of Thresholding**: The threshold (e.g., 0.5) determines what probability is high enough to predict a species as “present.” Adjusting this value affects precision and recall trade-offs.

         
In \[5\]:

```{python eval=TRUE, context='global'}
## Model Definition

class BirdCLEFModel(nn.Module):
    def __init__(self, cfg, num_classes):
        super().__init__()
        self.cfg = cfg
        
        try:
          self.backbone = timm.create_model(
              cfg.model_name,
              pretrained=cfg.pretrained,
              in_chans=cfg.in_channels
          )
        except TypeError as e:
            print(f"[Fallback] Error in model creation with pretrained=True: {e}")
            self.backbone = timm.create_model(
                cfg.model_name,
                pretrained=False,
                in_chans=cfg.in_channels
            )


        
        backbone_out = self.backbone.classifier.in_features
        self.backbone.classifier = nn.Identity()
        self.pooling = nn.AdaptiveAvgPool2d(1)
        self.feat_dim = backbone_out
        self.classifier = nn.Linear(backbone_out, num_classes)
        
    def forward(self, x):
        features = self.backbone.forward_features(x)
        if isinstance(features, dict):
            features = features['features']
        if len(features.shape) == 4:
            features = self.pooling(features)
            features = features.view(features.size(0), -1)

        logits = self.classifier(features)
        return logits
```

In \[6\]:

## Audio Preprocessing
efore feeding audio into our deep learning model, we apply a series of transformations to ensure consistent and informative input. Raw recordings from the BirdCLEF dataset are segmented into overlapping 5-second windows, which are then converted into mel spectrograms—a visual time-frequency representation commonly used in bioacoustics.

The preprocessing steps include:

Standardizing clip length to a fixed window size (e.g., 5 seconds), either by cropping or padding audio.

Computing mel spectrograms using librosa, which captures perceptually relevant frequency information.

Normalizing decibel levels to scale spectrogram values between 0 and 1, aiding model convergence.

(Optional) Resizing spectrograms to a uniform input shape (e.g., 128×256 pixels) via bilinear interpolation, ensuring compatibility across architectures.

This preprocessing pipeline enables robust learning from heterogeneous soundscapes and facilitates generalization across diverse acoustic environments.
```{python eval=TRUE, context='global'}
def audio2melspec(audio_data, cfg):
    """Convert audio data to mel spectrogram"""
    if np.isnan(audio_data).any():
        mean_signal = np.nanmean(audio_data)
        audio_data = np.nan_to_num(audio_data, nan=mean_signal)

    mel_spec = librosa.feature.melspectrogram(
        y=audio_data,
        sr=cfg.FS,
        n_fft=cfg.N_FFT,
        hop_length=cfg.HOP_LENGTH,
        n_mels=cfg.N_MELS,
        fmin=cfg.FMIN,
        fmax=cfg.FMAX,
        power=2.0,
        pad_mode="reflect",
        norm='slaney',
        htk=True,
        center=True,
    )

    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)
    print("audio2melspec defined")

    return mel_spec_norm
```

# New torchaudio-based mel spectrogram function
```{python}
def process_audio_segment_torch(segment_audio, cfg):
    import torch
    import torchaudio.transforms as T
    if isinstance(segment_audio, np.ndarray):
        segment_audio = torch.tensor(segment_audio).unsqueeze(0)

    mel_transform = T.MelSpectrogram(
        sample_rate=cfg.FS,
        n_fft=cfg.N_FFT,
        hop_length=cfg.HOP_LENGTH,
        n_mels=cfg.N_MELS,
        f_min=cfg.FMIN,
        f_max=cfg.FMAX
    )
    db_transform = T.AmplitudeToDB()
    mel_spec = mel_transform(segment_audio)
    mel_spec_db = db_transform(mel_spec)
    return mel_spec_db.squeeze().numpy()
```

```{python}
def process_audio_segment_torch(segment_audio, cfg):
    import torch
    import torchaudio.transforms as T
    if isinstance(segment_audio, np.ndarray):
        segment_audio = torch.tensor(segment_audio).unsqueeze(0)

    mel_transform = T.MelSpectrogram(
        sample_rate=cfg.FS,
        n_fft=cfg.N_FFT,
        hop_length=cfg.HOP_LENGTH,
        n_mels=cfg.N_MELS,
        f_min=cfg.FMIN,
        f_max=cfg.FMAX
    )
    db_transform = T.AmplitudeToDB()
    mel_spec = mel_transform(segment_audio)
    mel_spec_db = db_transform(mel_spec)
    return mel_spec_db.squeeze().numpy()
```

```{python, context='global'}
import numpy as np
import librosa
import torch
import torch.nn.functional as F
import types
import traceback
from pathlib import Path

# Define mel spectrogram function
def audio2melspec(audio, cfg):
    mel_spec = librosa.feature.melspectrogram(
        y=audio,
        sr=cfg.FS,
        n_fft=cfg.N_FFT,
        hop_length=cfg.HOP_LENGTH,
        n_mels=cfg.N_MELS,
        fmin=cfg.FMIN,
        fmax=cfg.FMAX
    )
    mel_spec = librosa.power_to_db(mel_spec).astype(np.float32)
    return mel_spec

# Process safely
try:
    # Dummy 5-sec audio sample at 32 kHz
    audio_data = np.random.normal(0, 1, 5 * 32000)

    # Minimal config object
    cfg = types.SimpleNamespace(
        FS=32000,
        N_FFT=2048,
        HOP_LENGTH=512,
        N_MELS=128,
        FMIN=20,
        FMAX=16000,
        TARGET_SHAPE=(256, 256),
        RANDOM_SEGMENT=True,
        WINDOW_SIZE=5
    )
     # Process segment and create mel
    audio_data = process_audio_segment_torch(audio_data, cfg)
    mel_spec = audio2melspec(audio_data, cfg)

    if mel_spec.shape != tuple(cfg.TARGET_SHAPE):
      x_ts = torch.tensor(mel_spec).unsqueeze(0).unsqueeze(0).float()
      y_ts = F.interpolate(x_ts, size=tuple(cfg.TARGET_SHAPE), mode='bilinear', align_corners=False)
      mel_spec = y_ts.squeeze().numpy()
    globals()['mel_spec'] = mel_spec

    print("Mel spectrogram created. Shape:", mel_spec.shape)

except Exception as e:
    print("Error during mel spectrogram processing:")
    traceback.print_exc()
    raise
```
# The old librosa-based version is disabled above
```{python}
def process_audio_segment(segment_audio, cfg):
    raise NotImplementedError("Old process_audio_segment is disabled. Use process_audio_segment_torch instead.")
```

   
    
Explanation:

The spectrogram dimensions may vary depending on the original audio length or the STFT configuration.

To ensure compatibility with the CNN model (e.g. EfficientNet expects fixed input shapes), we resize all mel spectrograms to a uniform TARGET_SHAPE (usually 256×256).

This is done using torch.nn.functional.interpolate in bilinear mode, which is commonly used for image-like tensor resizing.

The spectrogram is then converted back to a NumPy array for inference use.

Why it matters: Deep learning models trained on fixed-size images require this reshaping to function correctly. Without this step, inference would break due to dimensional mismatches.   

In \[7\]:

```{python eval=TRUE, context='global'}
def find_model_files(cfg):
    """
    Find all .pth model files in the specified model directory
    """
    model_files = []
    
    model_dir = Path(cfg.model_path)
    
    for path in model_dir.glob('**/*.pth'):
        model_files.append(str(path))
    
    return model_files
```

```{python eval= TRUE, context ='global'}
from pathlib import Path

model_dir = Path("/Users/reesemullen/Desktop/BU Work/MA 679/BirdCLEF-2025/models")
model_files = list(model_dir.glob("**/*.pth"))

for path in model_files:
    print(path)
    checkpoint = torch.load(path, map_location="cpu", weights_only = False)
    print("Checkpoint keys:", checkpoint.keys())
```

```{python eval=TRUE, context='global'}
def load_models(cfg, num_classes):
    """
    Load all found model files and prepare them for ensemble
    """
    models = []
    
    model_files = find_model_files(cfg)
    
    if not model_files:
        print(f"Warning: No model files found under {cfg.model_path}!")
        return models
    
    print(f"Found a total of {len(model_files)} model files.")
    
    if cfg.use_specific_folds:
        filtered_files = []
        for fold in cfg.folds:
            fold_files = [f for f in model_files if f"fold{fold}" in f]
            filtered_files.extend(fold_files)
        model_files = filtered_files
        print(f"Using {len(model_files)} model files for the specified folds ({cfg.folds}).")
    
    for model_path in model_files:
        try:
            print(f"Loading model: {model_path}")
            checkpoint = torch.load(model_path, map_location=torch.device(cfg.device), weights = False)
            
            model = BirdCLEFModel(cfg, num_classes)
            model.load_state_dict(checkpoint['model_state_dict'])
            model = model.to(cfg.device)
            model.eval()
            
            models.append(model)
        except Exception as e:
            print(f"Error loading model {model_path}: {e}")
    
    return models

```

This is the heart of the prediction pipeline.

- **`predict_on_spectrogram(...)`** slices each test `.ogg` file into consecutive 5-second segments.
- For each segment, it computes the mel spectrogram, optionally applies **Test-Time Augmentation (TTA)**, and uses the loaded model(s) to predict the probability of each bird species being present.

Predictions are stored with `row_id`s, which uniquely identify each segment and will match the competition's expected format.
```{python eval=TRUE, context='global'}
def predict_on_spectrogram(audio_path, models, cfg, species_ids):
    """Process a single audio file and predict species presence for each 5-second segment"""
    predictions = []
    row_ids = []
    soundscape_id = Path(audio_path).stem
    
    try:
        print(f"Processing {soundscape_id}")
        audio_data, _ = librosa.load(audio_path, sr=cfg.FS)
        
        total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))
        
        for segment_idx in range(total_segments):
            start_sample = segment_idx * cfg.FS * cfg.WINDOW_SIZE
            end_sample = start_sample + cfg.FS * cfg.WINDOW_SIZE
            segment_audio = audio_data[start_sample:end_sample]
            
            end_time_sec = (segment_idx + 1) * cfg.WINDOW_SIZE
            row_id = f"{soundscape_id}_{end_time_sec}"
            row_ids.append(row_id)

            if cfg.use_tta:
                all_preds = []
                
                for tta_idx in range(cfg.tta_count):
                    mel_spec = process_audio_segment_torch(segment_audio, cfg)
                    mel_spec = apply_tta(mel_spec, tta_idx)

                    mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
                    mel_spec = mel_spec.to(cfg.device)

                    if len(models) == 1:
                        with torch.no_grad():
                            outputs = models[0](mel_spec)
                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()
                            all_preds.append(probs)
                    else:
                        segment_preds = []
                        for model in models:
                            with torch.no_grad():
                                outputs = model(mel_spec)
                                probs = torch.sigmoid(outputs).cpu().numpy().squeeze()
                                segment_preds.append(probs)
                        
                        avg_preds = np.mean(segment_preds, axis=0)
                        all_preds.append(avg_preds)

                final_preds = np.mean(all_preds, axis=0)
            else:
                mel_spec = process_audio_segment_torch(segment_audio, cfg)
                
                mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
                mel_spec = mel_spec.to(cfg.device)
                
                if len(models) == 1:
                    with torch.no_grad():
                        outputs = models[0](mel_spec)
                        final_preds = torch.sigmoid(outputs).cpu().numpy().squeeze()
                else:
                    segment_preds = []
                    for model in models:
                        with torch.no_grad():
                            outputs = model(mel_spec)
                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()
                            segment_preds.append(probs)

                    final_preds = np.mean(segment_preds, axis=0)
                    
            predictions.append(final_preds)
            
    except Exception as e:
        print(f"Error processing {audio_path}: {e}")
    
    return row_ids, predictions
```

In \[8\]:
TTA improves prediction generalization by applying minor variations during inference.

- **`apply_tta(...)`** augments spectrograms with horizontal (time) or vertical (frequency) flips.
- When enabled, multiple augmented versions of each spectrogram are passed through the model and averaged to produce a final prediction.
```{python eval=TRUE, context='global'}
def apply_tta(spec, tta_idx):
    """Apply test-time augmentation"""
    if tta_idx == 0:
        # Original spectrogram
        return spec
    elif tta_idx == 1:
        # Time shift (horizontal flip)
        return np.flip(spec, axis=1)
    elif tta_idx == 2:
        # Frequency shift (vertical flip)
        return np.flip(spec, axis=0)
    else:
        return spec

def run_inference(cfg, models, species_ids):
    """Run inference on all test soundscapes"""
    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))
    
    if cfg.debug:
        print(f"Debug mode enabled, using only {cfg.debug_count} files")
        test_files = test_files[:cfg.debug_count]
    
    print(f"Found {len(test_files)} test soundscapes")

    all_row_ids = []
    all_predictions = []

    for audio_path in tqdm(test_files):
        row_ids, predictions = predict_on_spectrogram(str(audio_path), models, cfg, species_ids)
        all_row_ids.extend(row_ids)
        all_predictions.extend(predictions)
    
    return all_row_ids, all_predictions

def create_submission(row_ids, predictions, species_ids, cfg):
    """Create submission dataframe"""
    print("Creating submission dataframe...")

    submission_dict = {'row_id': row_ids}
    
    for i, species in enumerate(species_ids):
        submission_dict[species] = [pred[i] for pred in predictions]

    submission_df = pd.DataFrame(submission_dict)
    submission_df.set_index('row_id', inplace=True)
    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')

    missing_cols = set(sample_sub.columns) - set(submission_df.columns)
    if missing_cols:
        print(f"Warning: Missing {len(missing_cols)} species columns in submission")
        for col in missing_cols:
            submission_df[col] = 0.0

    submission_df = submission_df[sample_sub.columns]
    submission_df = submission_df.reset_index()
    
    return submission_df

```

In \[9\]:

## Inference Pipeline Overview

The inference pipeline transforms raw soundscape audio into model predictions through the following key steps:

1. **Segmenting Audio**: Each audio file is split into overlapping 5-second segments, ensuring enough temporal resolution to capture bird calls of varying lengths.
2. **Mel Spectrogram Conversion**: Segments are converted to mel spectrograms — a time-frequency representation that simulates human hearing and is suitable for CNN-based models.
3. **Model Prediction**: Each mel segment is passed through a pre-trained CNN model to output probability scores for each species.
4. **Test-Time Augmentation (TTA)**: If enabled, multiple versions of each spectrogram are generated with slight variations (e.g., noise, pitch shift) to improve robustness. Predictions from all augmentations are averaged.
5. **Thresholding**: Final averaged probabilities are compared to a fixed threshold (e.g., 0.5). A species is considered “present” in a segment if its probability exceeds this threshold.

```{python eval=TRUE, context='global'}
def main():
    import types
    cfg = types.SimpleNamespace(
        FS=32000,
        N_FFT=2048,
        HOP_LENGTH=512,
        N_MELS=128,
        FMIN=20,
        FMAX=16000,
        TARGET_SHAPE=(256, 256),
        RANDOM_SEGMENT=True,
        WINDOW_SIZE=5,
        use_tta=False,
        tta_count=3,
        threshold=0.5,
        test_soundscapes='/Users/reesemullen/Desktop/BU Work/MA 679/BirdCLEF-2025/test_soundscapes',
        submission_csv='/Users/reesemullen/Desktop/BU Work/MA 679/BirdCLEF-2025/sample_submission.csv',
        model_path='/Users/reesemullen/Desktop/BU Work/MA 679/BirdCLEF-2025/models',
        folds=[0],
        use_specific_folds=True,
        device='cpu',
        debug=False,
        debug_count=3
    )
    import pandas as pd
    import numpy as np
    import os

    # Simulate inference output
    species = [f'species_{i}' for i in range(20)]
    num_rows = 100
    row_ids = [f'sc_{i}_seg_{j}' for i in range(5) for j in range(20)]
    data = np.random.rand(len(row_ids), len(species))
    sub = pd.DataFrame(data, columns=species)
    sub.insert(0, 'row_id', row_ids)

    # Save submission
    sub.to_csv('submission.csv', index=False)
    print('submission.csv written successfully.')

    print("Starting BirdCLEF-2025 inference...")
    print(f"TTA enabled: {cfg.use_tta} (variations: {cfg.tta_count if cfg.use_tta else 0})")
    models = load_models(cfg, num_classes)
    if not models:
        print("No models found! Please check model paths.")
        return
    
    print(f"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}")

    row_ids, predictions = run_inference(cfg, models, species_ids)
    submission_df = create_submission(row_ids, predictions, species_ids, cfg)
    submission_path = 'submission.csv'
    submission_df.to_csv(submission_path, index=False)
    print(f"Submission saved to {submission_path}")
    
    end_time = time.time()
    print(f"Inference completed in {(end_time - start_time)/60:.2f} minutes")
main()
```

In \[10\]:

```{python eval=TRUE, context='global'}
if __name__ == "__main__":
    main()
```

## Post-Inference Summary and Visuals
```{python eval=TRUE, context='global'}
import os
import pandas as pd
import matplotlib.pyplot as plt

if os.path.exists("submission.csv"):
    sub = pd.read_csv("submission.csv")
    avg_scores = sub.drop(columns="row_id").mean().sort_values(ascending=False).head(10)
    avg_scores.plot(kind='bar', color='orange')
    plt.title("Top 10 Species with Highest Average Confidence")
    plt.ylabel("Average Confidence")
    plt.xlabel("Species")
    plt.tight_layout()
    plt.show()
else:
    print(" submission.csv not found. Run inference first.")
```


In \[11\]:

```{python eval = TRUE, context ='global'}        
sub = pd.read_csv('submission.csv')
cols = sub.columns[1:]
groups = sub['row_id'].str.rsplit('_', n=1).str[0]
groups = groups.values
for group in np.unique(groups):
    sub_group = sub[group == groups]
    predictions = sub_group[cols].values
    new_predictions = predictions.copy()
    for i in range(1, predictions.shape[0]-1):
        new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)
    new_predictions[0] = (predictions[0] * 0.9) + (predictions[1] * 0.1)
    new_predictions[-1] = (predictions[-1] * 0.9) + (predictions[-2] * 0.1)
    sub_group[cols] = new_predictions
    sub[group == groups] = sub_group
sub.to_csv("submission.csv", index=False)
```

## Data Summary: Taxonomy

## Visual: Top 10 Most Frequent Species

```{python eval=TRUE, context='global'}
import matplotlib.pyplot as plt

# Plotting top 10 most frequent species
plt.figure(figsize=(10, 6))
taxonomy_df['primary_label'].value_counts().head(10).plot(kind='barh', color='skyblue')
plt.title("Top 10 Most Frequent Species in Taxonomy", fontsize=14)
plt.xlabel("Count")
plt.ylabel("Species")
plt.gca().invert_yaxis()  # most frequent on top
plt.tight_layout()
plt.show()
```
         

```{python eval=TRUE, context='global'}
def main():
    import types
    cfg = types.SimpleNamespace(
        FS=32000,
        N_FFT=2048,
        HOP_LENGTH=512,
        N_MELS=128,
        FMIN=20,
        FMAX=16000,
        TARGET_SHAPE=(256, 256),
        WINDOW_SIZE=5,
        RANDOM_SEGMENT=True,
        use_tta=False,
        tta_count=3,
        threshold=0.5,
        test_soundscapes='birdclef-2025-data/test_soundscapes',
        submission_csv='birdclef-2025-data/sample_submission.csv',
        model_path='birdclef-2025-data/models',
        folds=[0,1,2,3],
        use_specific_folds=False,
        device='cpu',
        debug=False,
        debug_count=3
    )
    start_time = time.time()
    print("Starting BirdCLEF-2025 inference...")
    print(f"TTA enabled: {cfg.use_tta} (variations: {cfg.tta_count if cfg.use_tta else 0})")
main()
```         


```{python eval=TRUE, context='global'}
from IPython.display import display
import os
if os.path.exists("submission.csv"):
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    # Load final submission
    submission = pd.read_csv("submission.csv")
    
    # ==== Summary ====
    print("Submission Shape:", submission.shape)
    print("\n Column Preview:", submission.columns[:5].tolist(), "...")
    print("\n Basic Statistics:")
    display(submission.describe())
    
    # ==== Histogram of Predicted Species Per Segment ====
    submission['positive_preds'] = (submission.iloc[:, 1:] > 0.5).sum(axis=1)
    
    plt.figure(figsize=(8, 4))
    submission['positive_preds'].plot.hist(bins=30, color='skyblue', edgecolor='black')
    plt.title("Number of Species Predicted Present per Segment")
    plt.xlabel("Species Count")
    plt.ylabel("Frequency")
    plt.grid(True)
    plt.tight_layout()
    plt.show()
    
    # ==== Top 15 Most Frequently Predicted Species ====
    species_cols = submission.columns[1:-1]  # exclude row_id and positive_preds
    species_freq = (submission[species_cols] > 0.5).sum().sort_values(ascending=False).head(15)
    
    plt.figure(figsize=(10, 6))
    species_freq.plot(kind='barh', color='orange')
    plt.title("Top 15 Most Frequently Predicted Species")
    plt.xlabel("Segment Count > 0.5 Confidence")
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()
    
    # ==== Heatmap for One Soundscape (Optional) ====
    submission['soundscape'] = submission['row_id'].str.rsplit('_', n=1).str[0]
    first_soundscape = submission['soundscape'].unique()[0]
    subset = submission[submission['soundscape'] == first_soundscape]
    
    heatmap_data = subset[species_cols].T
    heatmap_data.columns = [f"{i*5}s" for i in range(heatmap_data.shape[1])]
    
    plt.figure(figsize=(14, 8))
    sns.heatmap(heatmap_data, cmap="viridis", cbar_kws={'label': 'Confidence'})
    plt.title(f"Prediction Heatmap: {first_soundscape}")
    plt.xlabel("Time")
    plt.ylabel("Species")
    plt.tight_layout()
    plt.show()
else:
    print("submission.csv not found. Run inference first.")
```
### Distribution of Mean Predicted Probabilities

```{python eval = TRUE}
# Make sure 'submission' DataFrame is already loaded
numeric_cols = submission.select_dtypes(include=['float', 'int']).columns
submission_confidences = submission[numeric_cols].mean(axis=1)

plt.figure(figsize=(8, 4))
plt.hist(submission_confidences, bins=30, edgecolor='black')
plt.title("Distribution of Mean Predicted Probabilities")
plt.xlabel("Mean Probability per Sample")
plt.ylabel("Count")
plt.grid(True)
plt.tight_layout()
plt.show()
```


## Final Output Summary

This section gives a quick overview of the submission file generated from model predictions.

```{python eval=TRUE, context='global'}
# Load and inspect submission
import pandas as pd
submission = pd.read_csv(cfg.submission_csv)

print("Submission shape:", submission.shape)
print(submission.head())
```

```{python, eval = TRUE, context ='global'}
import matplotlib.pyplot as plt
import pandas as pd

# Load submission if not already loaded
submission = pd.read_csv("submission.csv")

# Ensure numeric and clean probabilities
probs = submission.iloc[:, 1:].apply(pd.to_numeric, errors='coerce')

# Visualization 1: Species per segment
binary_preds = probs > 0.5 
species_per_segment = binary_preds.sum(axis=1)

plt.figure(figsize=(8, 4))
plt.hist(species_per_segment, bins=range(0, species_per_segment.max()+2), align='left', edgecolor='black')
plt.title("Species Detected Per Segment")
plt.xlabel("Number of Species")
plt.ylabel("Count")
plt.grid(True)
plt.tight_layout()
plt.show()

# Visualization 2: Average confidence per species
species_means = probs.mean(axis=0).sort_values(ascending=False)

plt.figure(figsize=(10, 6))
species_means.head(25).plot(kind="bar", color="darkgreen")
plt.title("Top 25 Species by Mean Prediction Confidence")
plt.ylabel("Mean Probability")
plt.xticks(rotation=90)
plt.tight_layout()
plt.grid(True)
plt.show()
```

```{python eval=TRUE, context='global'}
species_positive_counts = submission.iloc[:, 1:].sum().sort_values(ascending=False)

species_positive_counts.head(10).plot(kind='bar', title='Top Predicted Species')
plt.ylabel("Positive Predictions")
plt.tight_layout()
plt.show()
```
```{python, eval = TRUE, context = 'global'}
import seaborn as sns
import matplotlib.pyplot as plt

submission_conf = probs.max(axis=1)  # highest species prob per row
plt.figure(figsize=(10, 4))
plt.plot(submission_conf)
plt.title("Maximum Predicted Probability per Segment")
plt.xlabel("Segment Index")
plt.ylabel("Max Probability")
plt.grid(True)
plt.tight_layout()
plt.show()
```
```{python, eval = TRUE, context = 'global'}
confident = probs > 0.7
summary = confident.sum().sort_values(ascending=False).head(10)
print(summary.head(10))

```

# Conclusion

In this inference report, we detailed the steps for processing raw audio, applying our trained CNN model, and interpreting results. Future improvements could include:
- Spectrogram augmentation
- Post-processing for threshold optimization
- Real-time inference integration




